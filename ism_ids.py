# -*- coding: utf-8 -*-
"""ISM_IDS.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s_kiH0L7wCreTGjD58oiRtm0USC6qw39
"""

!pip install kaggle

! mkdir ~/.kaggle #create kaggle directory
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json

!kaggle datasets download -d hassan06/nslkdd

zip_path='/content/nslkdd.zip'

import zipfile

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    for member in zip_ref.namelist():
      zip_ref.extract(member)

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import seaborn as sns

# Dataset field names
datacols = ["duration","protocol_type","service","flag","src_bytes",
    "dst_bytes","land","wrong_fragment","urgent","hot","num_failed_logins",
    "logged_in","num_compromised","root_shell","su_attempted","num_root",
    "num_file_creations","num_shells","num_access_files","num_outbound_cmds",
    "is_host_login","is_guest_login","count","srv_count","serror_rate",
    "srv_serror_rate","rerror_rate","srv_rerror_rate","same_srv_rate",
    "diff_srv_rate","srv_diff_host_rate","dst_host_count","dst_host_srv_count",
    "dst_host_same_srv_rate","dst_host_diff_srv_rate","dst_host_same_src_port_rate",
    "dst_host_srv_diff_host_rate","dst_host_serror_rate","dst_host_srv_serror_rate",
    "dst_host_rerror_rate","dst_host_srv_rerror_rate","attack", "last_flag"]

# Load NSL_KDD train dataset
X_train = pd.read_table("KDDTrain+.txt", sep=",", names=datacols) # change path to where the dataset is located.
X_train = X_train.iloc[:,:-1] # removes an unwanted extra field

# Load NSL_KDD test dataset
X_test = pd.read_table("KDDTest+.txt", sep=",", names=datacols)
X_test = X_test.iloc[:,:-1]

print("Train size  : ",X_train.shape)
print("Test size   : ",X_test.shape)

print("Train Split : " ,round(X_train.shape[0]/(X_train.shape[0]+X_test.shape[0])*100))
print("Test Split  : " ,round(X_test.shape[0]/(X_train.shape[0]+X_test.shape[0])*100))

"""Combining X_train and X_test"""

df=pd.concat([X_train,X_test])
df.head()

df.shape

df.info()

df.isnull().sum()

cols=df.columns
cols

"""Numerical Columns Visualization"""

sns.distplot(df['duration'])

plt.figure(figsize=(15,8))
sns.barplot(x=df['duration'],y=df['attack'])
plt.xticks(rotation='vertical')#to rotate the labels like apple, hp, acer etc... vertical
plt.show()

sns.distplot(df['num_failed_logins'])

plt.figure(figsize=(15,8))
sns.barplot(x=df['num_failed_logins'],y=df['attack'])
plt.xticks(rotation='vertical')#to rotate the labels like apple, hp, acer etc... vertical
plt.show()

sns.distplot(df['dst_host_same_src_port_rate'])

plt.figure(figsize=(15,8))
sns.barplot(x=df['dst_host_same_src_port_rate'],y=df['attack'])
plt.xticks(rotation='vertical')#to rotate the labels like apple, hp, acer etc... vertical
plt.show()

sns.distplot(df['num_compromised'])

plt.figure(figsize=(15,8))
sns.barplot(x=df['num_compromised'],y=df['attack'])
plt.xticks(rotation='vertical')#to rotate the labels like apple, hp, acer etc... vertical
plt.show()

"""Categorical Columns Visualization"""

plt.figure(figsize=(15,8))
df['service'].value_counts().plot(kind='bar')

df['protocol_type'].value_counts().plot(kind='bar')

df['flag'].value_counts().plot(kind='bar')

plt.figure(figsize=(15,8))
df['attack'].value_counts().plot(kind='bar')

"""Categorical Column Analysis"""

categorical_data=df.select_dtypes(include=['object']).copy()
categorical_data

cat_col=categorical_data.columns

for i in cat_col:
  print(f'{i} - {len(categorical_data[i].unique())} - {categorical_data[i].unique()}')

target=df['attack']
target.shape

print(target)

df1=df.copy()

df1.drop(['attack'],axis=1,inplace=True)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

# encode the categorical attributes
cat = categorical_data.apply(encoder.fit_transform)
print(cat)

# 3 categories
# 1, 2 , 3

# 1 0 0
# 0 1 0
# 0 0 1

from sklearn import preprocessing
label_encoder = preprocessing.LabelEncoder()

for col in df1.columns:
   if df1[col].dtype == 'object':
      df1[col]= label_encoder.fit_transform(df1[col])

df1.head()

"""Correlation"""

print("Highest correlation per column")
corr_matrix = df1.corr()
np.fill_diagonal(corr_matrix.values, 0)

# high_corr_columns=[]

# find column with highest correlation for each column in the DataFrame
column1=[]
column2=[]
correlation_val=[]
for col in df1.columns:
    max_corr_col = corr_matrix[col].idxmax()
    max_corr_val = corr_matrix[col][max_corr_col]
    column1.append(col)
    column2.append(max_corr_col)
    correlation_val.append(max_corr_val)
    print(f"{col} - {max_corr_col} : {max_corr_val}")
    # if(max_corr_val>0.8):
    #   high_corr_columns.append(max_corr_col)

df_correlation=pd.DataFrame({
    'Column1':column1,
    'Column2': column2,
    'Correlation':correlation_val
})
df_correlation

# keep columns where there are more than 1 unique values
# df1 = df1[[col for col in df1 if df1[col].nunique() > 1]]
plt.figure(figsize=(25,25))
dataplot = sns.heatmap(df1.corr(), cmap="YlGnBu", annot=True)
plt.show()

"""From the heatmap, we can infer that:
- **num_compromised** is highly correlated with num_root column.
- **serror_rate** is highly correlated with srv_serror_rate, dst_host_serror_rate and dst_host_srv_serror_rate.
- **rerror_rate** is highly correlated with srv_rerror_rate, dst_host_rerror_rate and dst_host_srv_rerror_rate.
- **dst_host_same_srv_rate** is correlated with dst_host_srv_count

In general, highly correlated variables indicates redundant data and may lead to overfitting. It can also lead to issues with model interpretability and can also cause instability during classification.
"""

print("Correlation Values: \n")

corr1 = df1['num_root'].corr(df1['num_compromised'])
print(f'{"{:.4f}".format(corr1)} : num_root - num_compromised')

corr2 = df1['serror_rate'].corr(df1['srv_serror_rate'])
print(f'{"{:.4f}".format(corr2)} : serror_rate - srv_serror_rate')

corr3 = df1['serror_rate'].corr(df1['dst_host_serror_rate'])
print(f'{"{:.4f}".format(corr3)} : serror_rate - dst_host_serror_rate')

corr4 = df1['serror_rate'].corr(df1['dst_host_srv_serror_rate'])
print(f'{"{:.4f}".format(corr4)} : serror_rate - dst_host_srv_serror_rate')

corr5 = df1['rerror_rate'].corr(df1['srv_rerror_rate'])
print(f'{"{:.4f}".format(corr5)} : rerror_rate - srv_rerror_rate')

corr6 = df1['rerror_rate'].corr(df1['dst_host_rerror_rate'])
print(f'{"{:.4f}".format(corr6)} : rerror_rate - dst_host_rerror_rate')

corr7 = df1['rerror_rate'].corr(df1['dst_host_srv_rerror_rate'])
print(f'{"{:.4f}".format(corr7)} : rerror_rate - dst_host_srv_rerror_rate')

corr8 = df1['dst_host_same_srv_rate'].corr(df1['dst_host_srv_count'])
print(f'{"{:.4f}".format(corr8)} : dst_host_same_srv_rate - dst_host_srv_count')

"""Hence we will drop the following columns:

- num_root
- srv_serror_rate
- dst_host_serror_rate
- dst_host_srv_serror_rate
- srv_rerror_rate
- dst_host_rerror_rate
- dst_host_srv_rerror_rate
- dst_host_same_srv_rate
"""

high_corr_columns=['num_root','srv_serror_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate','srv_rerror_rate','dst_host_rerror_rate', 'dst_host_srv_rerror_rate','dst_host_same_srv_rate']

for col in high_corr_columns:
  df1.drop(col, axis = 1, inplace = True)

df1.head()

"""We also notice that num_outbounds_cmds have only a sigle value. It doesnt provide andy variablity and is not relevent to the analysis process. So we drop it."""

df1.drop(['num_outbound_cmds'], axis=1, inplace=True)

corr2 = df1.corr()
plt.figure(figsize=(15, 12))
sns.heatmap(corr2)

plt.show()

df1.head()

"""Normalization"""

from sklearn import preprocessing
numeric_columns = df1.select_dtypes(include=['int', 'float']).columns

scaler = preprocessing.MinMaxScaler()
df1[numeric_columns] = scaler.fit_transform(df1[numeric_columns])

df1.head()

df2=df1.copy()

"""Handling Outliers using Zscore"""

from scipy import stats
import numpy as np

for col in df2.columns:
  # calculate the Z-scores for each data point
  z_scores = np.abs(stats.zscore(df2[col]))

  # identify the outliers
  threshold = 2.5
  outliers = np.where(z_scores > threshold)[0]

  # remove or replace the outliers
  df2 = df2.reset_index(drop=True) # reset index
  df2.drop(outliers, axis=0)

df2

"""Handling Outliers using IQR"""

df3=df1.copy()

# create an array of data
for col in df3.columns:
  data = np.array(df3[col])
  # calculate the IQR
  q1, q3 = np.percentile(data, [25, 75])
  iqr = q3 - q1

  # identify the outliers
  threshold = 1.5
  lower_bound = q1 - (threshold * iqr)
  upper_bound = q3 + (threshold * iqr)
  outliers = np.where((data < lower_bound) | (data > upper_bound))[0]

  # remove or replace the outliers
  df3 = df3.reset_index(drop=True) # reset index
  df3 = df3.drop(outliers, axis=0)

df3

"""Handling Outliers using Winsorization"""

df4=df1.copy()

from scipy.stats.mstats import winsorize

# create an array of data
for col in df3.columns:
  # create an array of data
  data = np.array(df3[col])
  # winsorize the data
  winsorized_data = winsorize(data, limits=[0.05, 0.05])

winsorized_data

def val_count(x):
  return x.value_counts()

# Attack Class Distribution
attack_freq_df1 = df[['attack']].apply(val_count)

attack_freq_df1['frequency_percent'] = round((100 * attack_freq_df1 / attack_freq_df1.sum()),2)
attack_freq_df1

# Attack class bar plot
plt.figure(figsize=(15,8))
attack_freq_df1['frequency_percent'].plot(kind='bar')
plt.title('Attack Class Distribution', fontsize=14)

top_15=attack_freq_df1.iloc[:15]
top_15

# Attack class bar plot
plt.figure(figsize=(15,8))
attack_freq_df1['frequency_percent'].iloc[:15].plot(kind='bar')
plt.title('Attack Class Distribution', fontsize=14)

df4=pd.concat([df1,target],axis=1)
df4

attacks_selected = ['normal', 'neptune', 'satan', 'ipsweep', 'smurf', 'portsweep', 'nmap', 'back', 'guess_passwd', 'mscan', 'warezmaster', 'teardrop', 'warezclient', 'apache2', 'processtable']

df4['attack'].unique()

df4['attack'] = df4['attack'].astype(str)

df4.loc[:]

df4['attack'].unique()

mask=df4['attack'].isin(attacks_selected)

mask

df_filtered = df4.loc[mask, :]

df_filtered

df_filtered['attack'].unique()

import pandas as pd
from sklearn.decomposition import PCA


pca=PCA().fit(df_filtered.iloc[:,:-1])
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlabel('number of components')

# apply PCA with 12 components
pca = PCA(n_components=12)
pca.fit(df_filtered.iloc[:,:-1])
print(pca.explained_variance_ratio_)
print("Sum",sum(pca.explained_variance_ratio_))

# get the 12 most significant columns
most_significant_cols = df_filtered.iloc[:,:-1].columns[pca.explained_variance_.argsort()[::-1][:12]]

# create a new dataframe with the 12 most significant columns
df_reduced = df_filtered[most_significant_cols]
df_reduced

df_5=pd.DataFrame(pd.concat([df_reduced,df_filtered.iloc[:,-1]],axis=1))

df_5 # dataset with pca reduced features [12 features + target]

df5_x=df_filtered.iloc[:,:-1]
df5_y=df_filtered.iloc[:,-1]

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier();

# fit random forest classifier on the training set
rfc.fit(df5_x, df5_y);
# extract important features
score = np.round(rfc.feature_importances_,3)
importances = pd.DataFrame({'feature':df1.columns,'importance':score})
importances = importances.sort_values('importance',ascending=False).set_index('feature')
# plot importances
plt.rcParams['figure.figsize'] = (11, 4)
importances.plot.bar();

from sklearn.feature_selection import RFE
import itertools
rfc = RandomForestClassifier()

# create the RFE model and select 10 attributes
rfe = RFE(rfc, n_features_to_select=10)
rfe = rfe.fit(df5_x, df5_y)

# summarize the selection of the attributes
feature_map = [(i, v) for i, v in itertools.zip_longest(rfe.get_support(), df1.columns)]
selected_features = [v for i, v in feature_map if i==True]

selected_features

df_selected = df5_x[selected_features]

df_6=pd.DataFrame(pd.concat([df_selected,df5_y],axis=1))

df_6 # dataset with rfe selected features [10 features + target]

"""Train Test Split"""

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(df_6.iloc[:,:-1],df_6.iloc[:,-1],random_state=20,test_size=0.3)

print(X_train.shape, X_test.shape, y_train.shape ,y_test.shape)

from sklearn.metrics import f1_score, classification_report,confusion_matrix,accuracy_score,precision_score,recall_score

def calculateScores(y_test,predictions):
  print('Accuracy  {0:.2f}%'.format(100*accuracy_score(y_test, predictions)))
  print('Precision {0:.2f}%'.format(100*precision_score(y_test,predictions, average='weighted', zero_division=0)))
  print('Recall {0:.2f}%'.format(100*recall_score(y_test,predictions, average='weighted')))
  print('F1 score {0:.2f}%'.format(100*f1_score(y_test,predictions, average='weighted')))

scaler2 = preprocessing.MinMaxScaler()

scaler2.fit(X_train)

X_train_scaled = scaler2.transform(X_train)
X_test_scaled = scaler2.transform(X_test)

"""Train Models and Performance Evaluation

SL algorithms: ANN, SVM, RF, KNN, Logistic Regression

Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
reg = LogisticRegression(multi_class='multinomial', solver='lbfgs',max_iter=1000)
lr_model = reg.fit(X_train_scaled, y_train)
lr_predictions = lr_model.predict(X_test_scaled)
calculateScores(y_test,lr_predictions)

print(classification_report(y_test, lr_predictions, target_names=attacks_selected))

"""SVM"""

from sklearn.svm import SVC
svm = SVC(kernel='rbf')#gaussian - radial basis function
svm_model = svm.fit(X_train_scaled, y_train)
svm_predictions = svm_model.predict(X_test_scaled)
calculateScores(y_test,svm_predictions)

print(classification_report(y_test, svm_predictions, target_names=attacks_selected))

"""RF"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf_model=rf.fit(X_train_scaled, y_train)
rf_predictions = rf_model.predict(X_test_scaled)
calculateScores(y_test,rf_predictions)

print(classification_report(y_test, rf_predictions, target_names=attacks_selected))

"""KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
knn_model=knn.fit(X_train_scaled, y_train)
knn_predictions = knn_model.predict(X_test_scaled)
calculateScores(y_test,knn_predictions)

print(classification_report(y_test, knn_predictions, target_names=attacks_selected))

"""DBSCAN"""

# from sklearn.cluster import DBSCAN
# dbscan = DBSCAN(eps=0.5, min_samples=5)
# dbscan_labels = dbscan.fit_predict(X_train_scaled)

"""ANN"""

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
import tensorflow as tf
from keras import backend as K

# Define custom metrics
def f1score(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0)
    precision = true_positives / (predicted_positives + K.epsilon())
    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)
    recall = true_positives / (actual_positives + K.epsilon())
    f1score = 2 * ((precision * recall) / (precision + recall + K.epsilon()))
    return K.mean(f1score)

def recall(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)
    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)
    recall = true_positives / (actual_positives + K.epsilon())
    return K.mean(recall)

def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0)
    precision = true_positives / (predicted_positives + K.epsilon())
    return K.mean(precision)


# define the number of classes
num_classes = 15

# Define your model architecture
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(50, activation='sigmoid', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(15, activation='softmax')
])

# compile the model
opt = tf.keras.optimizers.Adam(learning_rate=0.01)

model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy',f1score,recall,precision])
model.summary()

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X_train_scaled, y_train)
X_test_resampled, y_test_resampled = sm.fit_resample(X_test_scaled, y_test)

#Label Encoding of Resampled Data
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

y_train_label_2 = le.fit_transform(y_resampled)
y_test_label_2 = le.transform(y_test_resampled)

y_train_encoded_2 = to_categorical(y_train_label_2, num_classes=15)
y_test_encoded_2 = to_categorical(y_test_label_2, num_classes=15)

history = model.fit(X_resampled, y_train_encoded_2, epochs=3, batch_size=256, validation_data=(X_test_resampled, y_test_encoded_2))

# evaluate the model
scores = model.evaluate(X_test_resampled, y_test_encoded_2, verbose=0)
print('Test loss:', scores[0])
print('Test accuracy:', scores[1])
print('Test f1score:', scores[2])
print('Test recall:', scores[3])
print('Test precision:', scores[4])

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense
from keras.utils import to_categorical
import tensorflow as tf
from keras import backend as K

# Define custom metrics
def f1score(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0)
    precision = true_positives / (predicted_positives + K.epsilon())
    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)
    recall = true_positives / (actual_positives + K.epsilon())
    f1score = 2 * ((precision * recall) / (precision + recall + K.epsilon()))
    return K.mean(f1score)

def recall(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)
    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)), axis=0)
    recall = true_positives / (actual_positives + K.epsilon())
    return K.mean(recall)

def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)), axis=0)
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)), axis=0)
    precision = true_positives / (predicted_positives + K.epsilon())
    return K.mean(precision)


# define the number of classes
num_classes = 15

# Define your model architecture
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(15, activation='softmax')
])

# compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy',f1score,recall,precision])
model.summary()

y_train

"""Upsampling
- SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation technique used to address class imbalance in machine learning problems.
"""

from imblearn.over_sampling import SMOTE
sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X_train_scaled, y_train)
X_test_resampled, y_test_resampled = sm.fit_resample(X_test_scaled, y_test)

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

y_train_label = le.fit_transform(y_train)
y_test_label = le.transform(y_test)

#Label Encoding of Resampled Data
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()

y_train_label_2 = le.fit_transform(y_resampled)
y_test_label_2 = le.transform(y_test_resampled)

X_train_scaled.shape

y_train_encoded = to_categorical(y_train_label, num_classes=15)
y_test_encoded = to_categorical(y_test_label, num_classes=15)

y_train_encoded_2 = to_categorical(y_train_label_2, num_classes=15)
y_test_encoded_2 = to_categorical(y_test_label_2, num_classes=15)

y_train_encoded

# train the model
# Convert target column to one-hot encoded labels
y_train_label

history = model.fit(X_train_scaled, y_train_encoded, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test_encoded))

# score = history.evaluate(X_test_scaled, y_test_encoded, verbose=0)
# print(score)

# train the model
history2 = model.fit(X_resampled, y_train_encoded_2, epochs=10, batch_size=32, validation_data=(X_test_resampled, y_test_encoded_2))

"""Improved accuracy by increasing no. of layers of ANN and epochs"""

# evaluate the model
score2 = model.evaluate(X_test_resampled, y_test_encoded_2, verbose=0)
# print(score2)
# print('Test loss:', score2[0])
print('Test accuracy:', score2[1])
# print('Test f1score:', score2[2])
# print('Test recall:', score2[3])
# print('Test precision:', score2[4])

y_pred = model.predict(X_test_resampled)
y_pred = np.argmax(y_pred, axis=1)

print(classification_report(y_test_label_2, y_pred, target_names=attacks_selected))